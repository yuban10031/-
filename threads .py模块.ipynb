{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8548a29d753d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'page-box house-lst-page-box'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 使用finalAll方法，获取指定标签和属性下的内容\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m#pages = [i.strip() for i in page[0].text.split('\\n')] # 抓取出每个区域的二手房链接中所有的页数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpages\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mtotal_pages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pages' is not defined"
     ]
    }
   ],
   "source": [
    "# 导入开发模块\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 定义空列表，用于创建所有的爬虫链接\n",
    "urls = []\n",
    "# 指定爬虫所需的上海各个区域名称\n",
    "citys = ['pudongxinqu','minhang','baoshan','xuhui','putuo','yangpu','changning','songjiang',\n",
    "         'jiading','huangpu','jinan','zhabei','hongkou','qingpu','fengxian','jinshan','chongming']\n",
    "\n",
    "# 基于for循环，构造完整的爬虫链接\n",
    "for i in citys:\n",
    "    url = 'http://sh.lianjia.com/ershoufang/%s/' %i\n",
    "    res = requests.get(url) # 发送get请求\n",
    "    res = res.text.encode(res.encoding).decode('utf-8') # 需要转码，否则会有问题\n",
    "    soup = BeautifulSoup(res,'html.parser') # 使用bs4模块，对响应的链接源代码进行html解析\n",
    "    page = soup.findAll('div',{'class':'page-box house-lst-page-box'}) # 使用finalAll方法，获取指定标签和属性下的内容\n",
    "    pages = [i.strip() for i in page[0].text.split('\\n')] # 抓取出每个区域的二手房链接中所有的页数\n",
    "    if len(pages) > 3:\n",
    "        total_pages = int(pages[-3])\n",
    "    else:\n",
    "        total_pages = int(pages[-2])\n",
    "    \n",
    "    for j in list(range(1,total_pages+1)): # 拼接所有需要爬虫的链接\n",
    "        urls.append('http://sh.lianjia.com/ershoufang/%s/d%s' %(i,j))\n",
    "\n",
    "# 创建csv文件，用于后面的保存数据\n",
    "file = open('lianjia.csv','w',encoding = 'utf-8')\n",
    "\n",
    "for url in urls: # 基于for循环，抓取出所有满足条件的标签和属性列表，存放在find_all中\n",
    "    res = requests.get(url)\n",
    "    res = res.text.encode(res.encoding).decode('utf-8')\n",
    "    soup = BeautifulSoup(res, 'html.parser')\n",
    "    find_all = soup.find_all(name = 'div', attrs = {'class':'info-panel'})\n",
    "\n",
    "    for i in list(range(len(find_all))): # 基于for循环，抓取出所需的各个字段信息\n",
    "        title = find_all[i].find('a')['title'] # 每套二手房的标语\n",
    "        res2 = find_all[i]\n",
    "        name = res2.find_all('div',{'class':'where'})[0].find_all('span')[0].text # 每套二手房的小区名称\n",
    "        room_type = res2.find_all('div',{'class':'where'})[0].find_all('span')[1].text # 每套二手房的户型\n",
    "        size = res2.find_all('div',{'class':'where'})[0].find_all('span')[2].text[:-3] # 每套二手房的面积\n",
    "\n",
    "\t\t# 采用列表解析式，删除字符串的首位空格\n",
    "        info = [i.strip() for i in res2.find_all('div',{'class':'con'})[0].text.split('\\n')]\n",
    "        region = info[1] # 每套二手房所属的区域\n",
    "        loucheng = info[2][2:] # 每套二手房所在的楼层\n",
    "        chaoxiang = info[5][2:] # 每套二手房的朝向\n",
    "        builtdate = info[-3][2:] # 每套二手房的建筑时间\n",
    "\n",
    "\t\t# 每套二手房的总价\n",
    "        price = find_all[i].find('div',{'class':'price'}).text.strip()[:-1]\n",
    "\t\t# 每套二手房的平方米售价\n",
    "        price_union = find_all[i].find('div',{'class':'price-pre'}).text.strip()[:-3]\n",
    "\n",
    "        #print(name,room_type,size,region,loucheng,chaoxiang,price,price_union,builtdate)\n",
    "\t\t# 将上面的各字段信息值写入并保存到csv文件中\n",
    "        file.write(','.join((name,room_type,size,region,loucheng,chaoxiang,price,price_union,builtdate))+'\\n')\n",
    "\n",
    "# 关闭文件（否则数据不会写入到csv文件中）\n",
    "file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
